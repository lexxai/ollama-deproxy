# Remote Ollama API endpoint (e.g., https://openwebui.example.com/ollama) (required)
REMOTE_URL=

# Authentication token for remote API (typically Bearer token format: "Bearer  YOUR_TOKEN_HERE")
REMOTE_AUTH_TOKEN=

# Logging level (e.g., DEBUG, INFO, WARNING, ERROR, CRITICAL) (default: INFO)
#LOG_LEVEL=INFO

# Enable HTTP/2 protocol for remote connections (default: True)
#REMOTE_URL_HTTP2=True

# Custom header to use for authentication (default: "Authorization")
#REMOTE_AUTH_HEADER=Authorization

# Local port to listen on (default: 11434)
#LOCAL_PORT=11434

# Request timeout for remote connections (in seconds; blank for no timeout)
#REMOTE_TIMEOUT=

# Stream responses from remote API (default: True)
#STREAM_RESPONSE=True

# Passthrough: Automatically decode `br` (Brotli) and `zstd` encoded responses (advanced, default: False)
#DECODE_RESPONSE=False

# Enable debugging for incoming requests (default: False)
#DEBUG_REQUEST=False

# If models are listed as numeric strings (e.g., "1", "2"), replace them with the corresponding model names from `ollama list` output (default: False)
# CORRECT_NUMBERED_MODEL_NAMES=False

# Cache for selected model endpoints is enabled by default.
#CACHE_ENABLED=true

# Maximum number of cached entries (default: 512).
#CACHE_MAXSIZE=512

# Cache TTL in seconds (default: 12 hours).
#CACHE_TTL=43200

# Hash algorithm used for cache keys.
# - auto: benchmark available algorithms on startup and pick the fastest for this platform
# - or set explicitly to one of: blake2s, blake2b, sha256
# Tip: after the first run, you can set the selected value here to skip auto-detection.
#HASH_ALGORITHM=auto
